{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "class Timer():\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.start_dt = None\n",
    "\n",
    "\tdef start(self):\n",
    "\t\tself.start_dt = dt.datetime.now()\n",
    "\n",
    "\tdef stop(self):\n",
    "\t\tend_dt = dt.datetime.now()\n",
    "\t\tprint('Time taken: %s' % (end_dt - self.start_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
    "\n",
    "    def __init__(self, filename, split, cols):\n",
    "        dataframe = pd.read_csv(filename)\n",
    "        i_split = int(len(dataframe) * split)\n",
    "        self.data_train = dataframe.get(cols).values[:i_split]\n",
    "        self.data_test  = dataframe.get(cols).values[i_split:]\n",
    "        self.len_train  = len(self.data_train)\n",
    "        self.len_test   = len(self.data_test)\n",
    "        self.len_train_windows = None\n",
    "\n",
    "    def get_test_data(self, seq_len, normalise):\n",
    "        '''\n",
    "        Create x, y test data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise reduce size of the training split.\n",
    "        '''\n",
    "        data_windows = []\n",
    "        for i in range(self.len_test - seq_len):\n",
    "            data_windows.append(self.data_test[i:i+seq_len])\n",
    "\n",
    "        data_windows = np.array(data_windows).astype(float)\n",
    "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
    "\n",
    "        x = data_windows[:, :-1]\n",
    "        y = data_windows[:, -1, [0]]\n",
    "        return x,y\n",
    "\n",
    "    def get_train_data(self, seq_len, normalise):\n",
    "        '''\n",
    "        Create x, y train data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise use generate_training_window() method.\n",
    "        '''\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for i in range(self.len_train - seq_len):\n",
    "            x, y = self._next_window(i, seq_len, normalise)\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "        return np.array(data_x), np.array(data_y)\n",
    "\n",
    "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
    "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
    "        i = 0\n",
    "        while i < (self.len_train - seq_len):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for b in range(batch_size):\n",
    "                if i >= (self.len_train - seq_len):\n",
    "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
    "                    yield np.array(x_batch), np.array(y_batch)\n",
    "                    i = 0\n",
    "                x, y = self._next_window(i, seq_len, normalise)\n",
    "                x_batch.append(x)\n",
    "                y_batch.append(y)\n",
    "                i += 1\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "    def _next_window(self, i, seq_len, normalise):\n",
    "        '''Generates the next data window from the given index location i'''\n",
    "        window = self.data_train[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:-1]\n",
    "        y = window[-1, [0]]\n",
    "        return x, y\n",
    "\n",
    "    def normalise_windows(self, window_data, single_window=False):\n",
    "        '''Normalise window with a base value of zero'''\n",
    "        normalised_data = []\n",
    "        window_data = [window_data] if single_window else window_data\n",
    "        for window in window_data:\n",
    "            normalised_window = []\n",
    "            for col_i in range(window.shape[1]):\n",
    "                normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n",
    "                normalised_window.append(normalised_col)\n",
    "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
    "            normalised_data.append(normalised_window)\n",
    "        return np.array(normalised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "\t\"\"\"A class for an building and inferencing an lstm model\"\"\"\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.model = Sequential()\n",
    "\n",
    "\tdef load_model(self, filepath):\n",
    "\t\tprint('[Model] Loading model from file %s' % filepath)\n",
    "\t\tself.model = load_model(filepath)\n",
    "\n",
    "\tdef build_model(self, configs):\n",
    "\t\ttimer = Timer()\n",
    "\t\ttimer.start()\n",
    "\n",
    "\t\tfor layer in configs['model']['layers']:\n",
    "\t\t\tneurons = layer['neurons'] if 'neurons' in layer else None\n",
    "\t\t\tdropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "\t\t\tactivation = layer['activation'] if 'activation' in layer else None\n",
    "\t\t\treturn_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "\t\t\tinput_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "\t\t\tinput_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "\n",
    "\t\t\tif layer['type'] == 'dense':\n",
    "\t\t\t\tself.model.add(Dense(neurons, activation=activation))\n",
    "\t\t\tif layer['type'] == 'lstm':\n",
    "\t\t\t\tself.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
    "\t\t\tif layer['type'] == 'dropout':\n",
    "\t\t\t\tself.model.add(Dropout(dropout_rate))\n",
    "\n",
    "\t\tself.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'])\n",
    "\n",
    "\t\tprint('[Model] Model Compiled')\n",
    "\t\ttimer.stop()\n",
    "\n",
    "\tdef train(self, x, y, epochs, batch_size, save_dir=\"./\"):\n",
    "\t\ttimer = Timer()\n",
    "\t\ttimer.start()\n",
    "\t\tprint('[Model] Training Started')\n",
    "\t\tprint('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n",
    "\t\t\n",
    "\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
    "\t\tcallbacks = [\n",
    "\t\t\tEarlyStopping(monitor='val_loss', patience=2),\n",
    "\t\t\tModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)\n",
    "\t\t]\n",
    "\t\tself.model.fit(\n",
    "\t\t\tx,\n",
    "\t\t\ty,\n",
    "\t\t\tepochs=int(epochs),\n",
    "\t\t\tbatch_size=int(batch_size),\n",
    "\t\t\tcallbacks=callbacks\n",
    "\t\t)\n",
    "\t\tself.model.save(save_fname)\n",
    "\n",
    "\t\tprint('[Model] Training Completed. Model saved as %s' % save_fname)\n",
    "\t\ttimer.stop()\n",
    "\n",
    "\tdef train_generator(self, data_gen, epochs, batch_size, steps_per_epoch, save_dir):\n",
    "\t\ttimer = Timer()\n",
    "\t\ttimer.start()\n",
    "\t\tprint('[Model] Training Started')\n",
    "\t\tprint('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size, steps_per_epoch))\n",
    "\t\t\n",
    "\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
    "\t\tcallbacks = [\n",
    "\t\t\tModelCheckpoint(filepath=save_fname, monitor='loss', save_best_only=True)\n",
    "\t\t]\n",
    "\t\tself.model.fit_generator(\n",
    "\t\t\tdata_gen,\n",
    "\t\t\tsteps_per_epoch=steps_per_epoch,\n",
    "\t\t\tepochs=epochs,\n",
    "\t\t\tcallbacks=callbacks,\n",
    "\t\t\tworkers=1\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tprint('[Model] Training Completed. Model saved as %s' % save_fname)\n",
    "\t\ttimer.stop()\n",
    "\n",
    "\tdef predict_point_by_point(self, data):\n",
    "\t\t#Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n",
    "\t\tprint('[Model] Predicting Point-by-Point...')\n",
    "\t\tpredicted = self.model.predict(data)\n",
    "\t\tpredicted = np.reshape(predicted, (predicted.size,))\n",
    "\t\treturn predicted\n",
    "\n",
    "\tdef predict_sequences_multiple(self, data, window_size, prediction_len):\n",
    "\t\t#Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n",
    "\t\tprint('[Model] Predicting Sequences Multiple...')\n",
    "\t\tprediction_seqs = []\n",
    "\t\tfor i in range(int(len(data)/prediction_len)):\n",
    "\t\t\tcurr_frame = data[i*prediction_len]\n",
    "\t\t\tpredicted = []\n",
    "\t\t\tfor j in range(prediction_len):\n",
    "\t\t\t\tpredicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "\t\t\t\tcurr_frame = curr_frame[1:]\n",
    "\t\t\t\tcurr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "\t\t\tprediction_seqs.append(predicted)\n",
    "\t\treturn prediction_seqs\n",
    "\n",
    "\tdef predict_sequence_full(self, data, window_size):\n",
    "\t\t#Shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "\t\tprint('[Model] Predicting Sequences Full...')\n",
    "\t\tcurr_frame = data[0]\n",
    "\t\tpredicted = []\n",
    "\t\tfor i in range(len(data)):\n",
    "\t\t\tpredicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "\t\t\tcurr_frame = curr_frame[1:]\n",
    "\t\t\tcurr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "\t\treturn predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tendulkar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/tendulkar/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[Model] Model Compiled\n",
      "Time taken: 0:00:00.339942\n",
      "[Model] Training Started\n",
      "[Model] 2 epochs, 32 batch size\n",
      "WARNING:tensorflow:From /Users/tendulkar/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "#configs = json.load(open('config.json', 'r'))\n",
    "configs = pd.read_json('config.json')\n",
    "\n",
    "data = DataLoader(\n",
    "\tos.path.join('data', configs['data']['filename']),\n",
    "\tconfigs['data']['train_test_split'],\n",
    "\tconfigs['data']['columns']\n",
    ")\n",
    "\n",
    "model = Model()\n",
    "model.build_model(configs)\n",
    "x, y = data.get_train_data(\n",
    "\tseq_len = configs['data']['sequence_length'],\n",
    "\tnormalise = configs['data']['normalise']\n",
    ")\n",
    "\n",
    "model.train(\n",
    "\tx,\n",
    "\ty,\n",
    "\tepochs = int(2),\n",
    "\tbatch_size = int(32)\n",
    ")\n",
    "\n",
    "x_test, y_test = data.get_test_data(\n",
    "\tseq_len = configs['data']['sequence_length'],\n",
    "\tnormalise = configs['data']['normalise']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
